## 数据挖掘实验报告

### 实验要求

本次实验是一个回归任务，在规模较大的数据集上实现随机森林算法进行回归预测，要求：

- 实现随机森林算法
- 实现算法并行化
- 体现算法的Cache友好

实验报告将依据上述内容依次进行论述，并展示算法的实验结果在Kaggle上面的效果。

---

### 算法原理

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本实验使用CART树上的集成来实现随机森林，CART（分类回归树）是决策树的一种类型，通过将空间用超平面根据某种标准进行划分，进而将特征空间不断划分为互不相交的区域，在输入测试数据进行决策时，会根据样本的特征值进行判断，最终会落入其中的一个区域中。

#### 损失函数

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本实验用CART树来进行回归，回归树会使得落入某个区域的样本的值归类到某个特定的值，且在生成的过程中使用平方损失函数，对于回归树$f(x)$，输入特征数据$x$，标准值$y$：
$$
Loss(y,f(x))=(f(x)-y)^2
$$

#### 节点划分 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在定义了损失函数以后，在构造回归树中，数据集$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m) \}$，其中$x \in R^d$和特征集$R=\lbrace R_1,R_2,\cdots,R_d \rbrace$，遍历第$i \in [1,d]$维特征，对于每个选定的特征，若数据集只有一个元素，则选取该元素的标签值作为该区域的返回值，否则产生候选分类值$Y=\{ \frac{y_i+y_{i+1}}{2} \}$，$i \in [1,m-1]$，并遍历候选分类值。对于每个选定的特征$R_i$和候选分类值$Y_i$，将数据划分为两部分，$D_{1}(R_i,Y_i)=\lbrace x | x \le Y_i \rbrace$，$D_{2}(R_i,Y_i)=\lbrace x|x>Y_i\rbrace$，即小于等于候选分类值的分为一堆，大于候选分类值的分为一堆，然后分别计算两堆数据的平均值：
$$
\hat{c}_{m}=\frac{1}{N_m}\sum_{x_i \in (R_i,Y_i)}y_i,x \in D_m,m=1,2
$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计算出左右两堆数据的平均值以后，计算该节点的$Loss$值，选择$Loss$值最小的特征和划分值进行划分，即目标函数定义为：
$$
\min_{R_i,Y_i}[\min_{c1}\sum_{x_i \in D_1(R_i,Y_i)}(y_i-\hat{c}_1)^2+\min_{c2}\sum_{x_i \in D_2(R_i,Y_i)}(y_i-\hat{c}_2)^2]
$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;划分结束后，将特征$R_i$从特征集$R$中剔除，即$R \leftarrow R / {R_i}$. 然后重复进行节点的划分即可，当满足划分的终止条件时，停止划分，若此时CART将特征空间划分为$M$个区域$K_1,K_2,\cdots,K_M$，则决策树可以表示为：
$$
f(x)=\sum_{m=1}^{M}\hat{c}_mI(x \in R_m)
$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上述公式表示的意思是，对于每个输入的数据$x \in R_m$，根据$I=\lbrace0,1\rbrace$来判定有无，进而得到回归预测值，最后划分的大概示意图如下所示：

![1561189807249](C:\Users\Alva\AppData\Roaming\Typora\typora-user-images\1561189807249.png)

#### CART 伪代码



#### 随机森林

在完善生成树的算法以后，就采用集成的思想来生成随机森林，随机森林生成的过程如下：

- 从原始样本中采用有放回抽样选取$n$个样本
- 对$n$个样本选取$m$个特征中的$k$个，建立决策树
- 重复上述操作$m$遍，生成$m$个决策树
- 根据输入样例，获得输出结果

其中，根据$[Breiman,2001a]$，$k=log_2m$.

---

### 并行化实现

---

### Cache优好的体现

---

### 实现结果